{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linnaeus University\n",
    "## Introduction to Machine learning, 25VT-2DV516\n",
    "## Assignment 1\n",
    "\n",
    "**Name:** Martim Oliveira\n",
    "\n",
    "**Email:** mo223tz@student.lnu.se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment you will handle four exercises related to the k-Nearest Neighbors algorithm.\n",
    "The main purpose is to get you up and running using Python, NumPy and Matplotlib. \n",
    "The library Scipy will be used specifically in Exercise 3, part 2.\n",
    "\n",
    "## Submission Instructions\n",
    "\n",
    "All exercises are individual. We expect you to submit a zip file with this notebook with your solutions and the MachineLearning.py with the models implemented. \n",
    "You must normalize your data before doing anything with your data.\n",
    "When grading your assignments we will in addition to functionality also take into account code quality. \n",
    "We expect well structured and efficient solutions. \n",
    "Finally, keep all your files in a single folder named as username_A1 and submit a zipped version of this folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Models implementation and testing (All Mandatory)\n",
    "\n",
    "1. Implement all the methods in the abstract classes **KNNRegressionModel** and **KNNClassificationModel** in the MachineLearningModel.py file. \n",
    "As the names suggest, you must implement the Regression (slide 30) and Classification (slide 24) versions of the KNN algorithm and you must follow the algorithms stated in the slides. \n",
    "* Both models must use the Euclidean distance as the distance function (*Tip: Code smart by implementing an auxiliary method _euclidian_distance() in the MachineLearningModel.py file*).\n",
    "* The evaluate() function for the **KNNRegressionModel** must implement the Mean Squared Error (MSE)\n",
    "* The evaluate() function for the **KNNClassificationModel** must count the number of correct predictions.\n",
    "\n",
    "2. Use the *Polynomial200.csv* dataset to show that all your methods for the **KNNRegressionModel** is working as expected. You must produce a similar figure to the one in slide 31. Instructions to produce the figure are present in the slide. You must show the effects of using k = 3, 5, 7 and 9 and discuss your findings on the figure produced.\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "Note: Please read instructions and further information on README file.\n",
    "\n",
    "After implementing the methods inherited from the abstract class, I've added one method to create the KNNRegression Plot representation. \n",
    "\n",
    "\n",
    "![KNNRegression_K=3](KNNRegression_Polynomial_3.png)\n",
    "\n",
    "- **When k = 3** we can observe that the line is quite sensitive to variations in the data\n",
    "- Follows the points closely - might be overfitting? \n",
    "- We can observer sharp peaks\n",
    "\n",
    "![KNNRegression_K=5](KNNRegression_Polynomial_5.png)\n",
    "\n",
    "- **When k = 5** we see a smoother line than k = 3\n",
    "- The line has less fluctuations\n",
    "- Overall balance, following the data and with smoother noise\n",
    "- Potential overfitting reduced, compared with k = 3\n",
    "\n",
    "![KNNRegression_K=7](KNNRegression_Polynomial_7.png)\n",
    "\n",
    "-  **When k = 7** it is smoother than the previous k-value\n",
    "- There is a reduced impact of individual points\n",
    "- More stable predictions\n",
    "\n",
    "![KNNRegression_K=9](KNNRegression_Polynomial_9.png)\n",
    "\n",
    "-  **When k = 9** we have the smoothest line of all observed k's\n",
    "- Most stable predicitons\n",
    "- Least sensitive to individual points\n",
    "\n",
    "**General Observations**\n",
    "1. Plots show the same overall trend:\n",
    "    - Sharp increse betweeen (0.0-0.2)\n",
    "    - Relatively stable between (0.2-0.8)\n",
    "    - Last increase at (0.8-1.0)\n",
    "2. Larger k-values produce smoother results.\n",
    "\n",
    "**Bellow we can see the implementation of the main methods - Euclidean distance calculation, predict and evaluate.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(self, point, data):\n",
    "        \"\"\"\n",
    "        Calculate the Euclidean distance between a point and the dataset points.\n",
    "        Euclidean equation: sqrt((X₂-X₁)²+(Y₂-Y₁)²) where:\n",
    "        X₂ = New entry's data.\n",
    "        X₁= Existing entry's data.\n",
    "        Y₂ = New entry's data.\n",
    "        Y₁ = Existing entry's data.\n",
    "        \"\"\"\n",
    "        point = np.array(point)\n",
    "        data = np.array(data)\n",
    "        return np.sqrt((point - data) ** 2) \n",
    "\n",
    "def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        The predictions are made by averaging the target variable of the k nearest neighbors.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like): Features of the new data.\n",
    "\n",
    "        Returns:\n",
    "        predictions (array-like): Predicted values.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for row in X:\n",
    "            distances = self.euclidean_distance(row, self.X_train)\n",
    "            sorted_distances = np.argsort(distances)\n",
    "            top_k_rows = sorted_distances[:self.k]\n",
    "            mean_value = np.mean(self.y_train[top_k_rows])\n",
    "            predictions.append(mean_value)\n",
    "        return predictions\n",
    "\n",
    "def evaluate(self, y_true, y_predicted):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the given data.\n",
    "        You must implement this method to calculate the Mean Squared Error (MSE) between the true and predicted values.\n",
    "        The MSE is calculated as the average of the squared differences between the true and predicted values.        \n",
    "\n",
    "        Parameters:\n",
    "        y_true (array-like): True target variable of the data.\n",
    "        y_predicted (array-like): Predicted target variable of the data.\n",
    "\n",
    "        Returns:\n",
    "        score (float): Evaluation score.\n",
    "        \"\"\"\n",
    "        return np.mean((np.array(y_true) - np.array(y_predicted)) ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Use the *IrisDataset.csv* dataset to show that all your methods for the **KNNClassificationModel** is working as expected. You must produce a similar figure to the one in slide 28. Instructions on how to produce the figure are given in the slide. You must choose 2 input variables only to produce the figure (they do not need to match the figure in the slide). You must show the effects of using k = 3, 5, 7, and 9 and discuss the figure produced.\n",
    "\n",
    "**Tips**\n",
    "\n",
    "* Check the function *np.meshgrid* from numpy to create the samples.\n",
    "* Check the function *plt.contourf* for generating the countours. \n",
    "* There are many tutorials online to produce this figure. Find one that most suits you.\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "**Key Points**\n",
    "\n",
    "- It is clear that Iris-Setosa (blue points) is separated from the other species, clustured in the lower left area\n",
    "- Iris-Versicolor and Virginica show some overlap in the middle region\n",
    "- The model appear to perform well for Iris-Setosa with any k-value\n",
    "- When k = 3 I would say that the model it's overfitting. However, for k = 5 and k = 7 I do have doubts; k = 5 seems the best model, although they have  similar baheviours. \n",
    "\n",
    "![Mesh Decision Boundary k = 3](MeshGrid_Iris_3.png)\n",
    "\n",
    "![Mesh Decision Boundary k = 5](MeshGrid_Iris_5.png)\n",
    "\n",
    "![Mesh Decision Boundary k = 7](MeshGrid_Iris_7.png)\n",
    "\n",
    "- K = 9 it is clearly an underfitting model. \n",
    "\n",
    "![Mesh Decision Boundary k = 9](MeshGrid_Iris_9.png)\n",
    "\n",
    "**Bellow we can see the implementation of the main methods - Euclidean distance calculation, predict, evaluate, and method to create Mesh Decision Boundary Visualization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_rows(self, lst):\n",
    "        \"\"\"\n",
    "        Get most frequent class from a list of labels.\n",
    "        \"\"\"\n",
    "        # Convert numpy arrays to list of strings\n",
    "        return max(set(lst), key=list(lst).count)\n",
    "\n",
    "def euclidean_distance(self, point, data):\n",
    "        \"\"\"\n",
    "        Calculate the Euclidean distance between a point and the dataset points.\n",
    "        Euclidean equation: sqrt((X₂-X₁)²+(Y₂-Y₁)²) where:\n",
    "        X₂ = New entry's data.\n",
    "        X₁= Existing entry's data.\n",
    "        Y₂ = New entry's data.\n",
    "        Y₁ = Existing entry's data.\"\"\"\n",
    "        \n",
    "        point = np.array(point)\n",
    "        data = np.array(data)\n",
    "        return np.sqrt(np.sum((point - data) ** 2, axis=1))\n",
    "    \n",
    "def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model using the given training data.\n",
    "        In this case, the training data is stored for later use in the prediction step.\n",
    "        The model does not need to learn anything from the training data, as KNN is a lazy learner.\n",
    "        The training data is stored in the class instance for later use in the prediction step.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like): Features of the training data.\n",
    "        y (array-like): Target variable of the training data.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.X_train = np.array(X)\n",
    "        self.y_train = np.array(y)\n",
    "\n",
    "def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        The predictions are made by taking the mode (majority) of the target variable of the k nearest neighbors.\n",
    "        \n",
    "        Parameters:\n",
    "        X (array-like): Features of the new data.\n",
    "\n",
    "        Returns:\n",
    "        predictions (array-like): Predicted values.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        X = np.array(X)\n",
    "        for row in X:\n",
    "            distances = self.euclidean_distance(row, self.X_train)\n",
    "            sorted_distances = np.argsort(distances)\n",
    "            top_k_rows = sorted_distances[:self.k]\n",
    "            neighbors = self.y_train[top_k_rows]\n",
    "            predictions.append(self.most_common_rows(neighbors))\n",
    "        return predictions\n",
    "        \n",
    "\n",
    "def evaluate(self, y_true, y_predicted):\n",
    "        \"\"\"\n",
    "        Evaluate the model on the given data.\n",
    "        You must implement this method to calculate the total number of correct predictions only.\n",
    "        Do not use any other evaluation metric.\n",
    "\n",
    "        Parameters:\n",
    "        y_true (array-like): True target variable of the data.\n",
    "        y_predicted (array-like): Predicted target variable of the data.\n",
    "\n",
    "        Returns:\n",
    "        score (float): Evaluation score.\n",
    "        \"\"\"\n",
    "        correct_predictions = 0\n",
    "        for i in range(len(y_true)):\n",
    "            # Handle nested predictions\n",
    "            pred = y_predicted[i]\n",
    "            if isinstance(pred, (list, np.ndarray)):\n",
    "                # For NumPy arrays, take the first element and convert to string\n",
    "                if isinstance(pred, np.ndarray):\n",
    "                    pred = str(pred[0])\n",
    "                else:\n",
    "                    pred = pred[0]\n",
    "            if y_true[i] == pred:\n",
    "                correct_predictions += 1\n",
    "        return correct_predictions\n",
    "    \n",
    "def knn_mesh_decision_boundary(self, X, y, ax):\n",
    "        \"\"\"\n",
    "        Create a visualization of the KNN classification model\n",
    "        \"\"\"\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(y)\n",
    "        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "        xx, yy = np.meshgrid(\n",
    "            np.linspace(x_min, x_max, 200),\n",
    "            np.linspace(y_min, y_max, 200)\n",
    "        )\n",
    "        grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "        Z = self.predict(grid_points)\n",
    "        Z_encoded = label_encoder.transform(Z)\n",
    "        Z_encoded = Z_encoded.reshape(xx.shape)\n",
    "    \n",
    "        # Plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.contourf(xx, yy, Z_encoded, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y_encoded, edgecolor='k', s=30, cmap=plt.cm.coolwarm)\n",
    "        plt.title(\"Mesh decision boundary for the Iris2D dataset using 5-NN\")\n",
    "        plt.xlabel(\"Sepal length (cm)\")\n",
    "        plt.ylabel(\"Sepal width (cm)\")\n",
    "\n",
    "        # Legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [Patch(facecolor=plt.cm.coolwarm(i / 2), label=cls) for i, cls in enumerate(label_encoder.classes_)]\n",
    "        plt.legend(handles=legend_elements, loc='upper right')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: KNN Regression (Mandatory)\n",
    "\n",
    "1. (Mandatory) Create a procedure to repeat 10 times the following strategy.\n",
    "* Use the values for k = 3, 5, 7, 9, 11, 13 and 15.\n",
    "* Split your dataset randomly into 80% for training, and 20% testing. Use 10 different seeds for splitting the data.\n",
    "* Evaluate (MSE implemented in your class) your **KNNRegressionModel** for each k in the **test set** and store the result. \n",
    "* Plot a barchart with these results.\n",
    "\n",
    "Which k gives the best regression? Motivate your answer!\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "After analyzing the Bar Chart, we can conclude some key points:\n",
    "- k = 3 gives us the highest MSE which suggests that has the poorest results. \n",
    "- This analysis suggest that k = 11 is the best choice for this task. \n",
    "- One can observe that there is a performance degradation when k is small and very large - too few neighbours leading to overfitting and too many to underfitting. \n",
    "\n",
    "![KNN Regression Bar Chart](KNNRegressionBarChart.png)\n",
    "\n",
    "\n",
    "\n",
    "**Bellow you can observe the function created for the problem.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_regression_experiment():\n",
    "    \"\"\"\n",
    "    Run the KNN regression with:\n",
    "    - 10 repetitions\n",
    "    - k values: 3, 5, 7, 9, 11, 13, 15\n",
    "    - 80-20 train-test split\n",
    "    - Different random seeds for each repetition\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Running KNN Regression Problem 2 ===\")\n",
    "    \n",
    "    # Load and prepare the polynomial dataset\n",
    "    polynomial = pd.read_csv('Polynomial200_normalized.csv')\n",
    "    X = polynomial['x'].values\n",
    "    y = polynomial['y'].values\n",
    "    \n",
    "    # Requirements \n",
    "    k_values = [3, 5, 7, 9, 11, 13, 15]\n",
    "    n_repetitions = 10\n",
    "    test_size = 0.2\n",
    "    random_seeds = range(42, 42 + n_repetitions)  # 10 different seeds\n",
    "    \n",
    "    results = np.zeros((n_repetitions, len(k_values)))\n",
    "    \n",
    "    for rep_idx, seed in enumerate(random_seeds):\n",
    "        print(f\"\\nRepetition {rep_idx + 1}/10\")\n",
    "        \n",
    "        # Split data with current seed\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=seed\n",
    "        )\n",
    "        \n",
    "        # Test each k value\n",
    "        for k_idx, k in enumerate(k_values):\n",
    "            print(f\"Testing k={k}\")\n",
    "            model = KNNRegressionModel(k=k)\n",
    "            model.fit(X_train, y_train)\n",
    "            predictions = model.predict(X_test)\n",
    "            mse = model.evaluate(y_test, predictions)\n",
    "            results[rep_idx, k_idx] = mse\n",
    "            print(f\"MSE for k={k}: {mse:.6f}\")\n",
    "    \n",
    "    # Calculate mean and std of MSE for each k\n",
    "    mean_mse = np.mean(results, axis=0)\n",
    "    std_mse = np.std(results, axis=0)\n",
    "    \n",
    "    # Create Bar Chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(k_values))\n",
    "    plt.bar(x, mean_mse, yerr=std_mse, capsize=5)\n",
    "    plt.xlabel('k value')\n",
    "    plt.ylabel('Mean Squared Error (MSE)')\n",
    "    plt.title('KNN Regression BarChart')\n",
    "    plt.xticks(x, k_values)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(mean_mse):\n",
    "        plt.text(i, v + std_mse[i], f'{v:.6f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.savefig('KNNRegressionBarChart.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\n===== Results =====\")\n",
    "    print(\"\\nMean MSE for each k value:\")\n",
    "    for k, mean, std in zip(k_values, mean_mse, std_mse):\n",
    "        print(f\"k={k}: {mean:.6f} ± {std:.6f}\")\n",
    "    \n",
    "    best_k_idx = np.argmin(mean_mse)\n",
    "    best_k = k_values[best_k_idx]\n",
    "    print(f\"\\nBest k value: {best_k} (MSE: {mean_mse[best_k_idx]:.6f} ± {std_mse[best_k_idx]:.6f})\")\n",
    "    print(f\"- Worst k value: {k_values[np.argmax(mean_mse)]} (MSE: {np.max(mean_mse):.6f})\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: KNN Classification (1 Mandatory , 1 Non-Mandatory)\n",
    "\n",
    "1. **(Mandatory)** Using the **IrisDataset.csv**, find the best combination of two features that produces the best model using **KNNClassificationModel**.\n",
    "* You must try all combinations of two features, and for k = 3, 5, 7, and 9.\n",
    "* You must use plots to support your answer.\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "- It is clearly that the pair {PetalLengthCm and PetalWidthCm} is the best combinatioun. \n",
    "- The created Scatter Plot is quite similar to the original one - we can observe distinct clusters between Setosa and the other two species. \n",
    "- Iris Versicolor and Virigica overlap, having more similar dimensions.\n",
    "\n",
    "![Scatter Plot](scatter_PetalLengthCm_PetalWidthCm.png)\n",
    "\n",
    "**Createad a specific function inside Run_models.py to solve this problem**\n",
    "\n",
    "The function returns the results for each possible pair of species and for different k-values.\n",
    "\n",
    "Best combination:\n",
    "Features: ('PetalLengthCm', 'PetalWidthCm')\n",
    "k value: 5\n",
    "Accuracy: 0.9667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_features_classification():\n",
    "    \"\"\"\n",
    "    Function to find the best combination of two features for the Iris dataset classification.\n",
    "    Tests all possible pairs of features with different k values.\n",
    "    Creates visualization for each combination.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Finding Best Feature Combination for KNN Classification ===\")\n",
    "\n",
    "    # Load and prepare data\n",
    "    df = pd.read_csv('IrisDataset_normalized.csv')\n",
    "    features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n",
    "    y = np.array(df['species'])  # Convert to numpy array\n",
    "    \n",
    "    # Automatically generate all possible pairs of features\n",
    "    feature_pairs = list(combinations(features, 2))\n",
    "    \n",
    "    k_values = [3, 5, 7, 9]\n",
    "    n_splits = 5  \n",
    "    results = {}  \n",
    "    \n",
    "    for pair in feature_pairs:\n",
    "        print(f\"\\nTesting feature pair: {pair}\")\n",
    "        X = df[list(pair)].values  # Convert to numpy array\n",
    "        pair_results = np.zeros((len(k_values), n_splits))\n",
    "        \n",
    "        for split_idx in range(n_splits):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42 + split_idx\n",
    "            )\n",
    "            \n",
    "            for k_idx, k in enumerate(k_values):\n",
    "                model = KNNClassificationModel(k=k)\n",
    "                model.fit(X_train, y_train)\n",
    "                predictions = model.predict(X_test)\n",
    "                accuracy = model.evaluate(y_test, predictions) / len(y_test)\n",
    "                pair_results[k_idx, split_idx] = accuracy\n",
    "        \n",
    "        results[pair] = np.mean(pair_results, axis=1)\n",
    "        \n",
    "        # Create scatter plot for this feature pair\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        unique_species = np.unique(y)\n",
    "        colors = ['blue', 'red', 'green']\n",
    "        \n",
    "        for species, color in zip(unique_species, colors):\n",
    "            mask = df['species'] == species\n",
    "            plt.scatter(\n",
    "                df[pair[0]][mask], \n",
    "                df[pair[1]][mask],\n",
    "                c=color,\n",
    "                label=species,\n",
    "                alpha=0.6\n",
    "            )\n",
    "        \n",
    "        plt.xlabel(pair[0])\n",
    "        plt.ylabel(pair[1])\n",
    "        plt.title(f'Scatter Plot: {pair[0]} vs {pair[1]}\\nBest Accuracy: {np.max(results[pair]):.4f}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(f'scatter_{pair[0]}_{pair[1]}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Find best combination\n",
    "    best_pair = max(results.items(), key=lambda x: np.max(x[1]))\n",
    "    best_k_idx = np.argmax(best_pair[1])\n",
    "    best_k = k_values[best_k_idx]\n",
    "    \n",
    "    print(\"\\n===== Results =====\")\n",
    "    print(\"\\nAccuracy for each feature combination:\")\n",
    "    for pair, accuracies in results.items():\n",
    "        print(f\"\\n{pair}:\")\n",
    "        for k, acc in zip(k_values, accuracies):\n",
    "            print(f\"k={k}: {acc:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest combination:\")\n",
    "    print(f\"Features: {best_pair[0]}\")\n",
    "    print(f\"k value: {best_k}\")\n",
    "    print(f\"Accuracy: {best_pair[1][best_k_idx]:.4f}\")\n",
    "    \n",
    "    # Summary Bar Chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(feature_pairs))\n",
    "    \n",
    "    # Plot average accuracy for each pair\n",
    "    avg_accuracies = [np.max(results[pair]) for pair in feature_pairs]\n",
    "    bars = plt.bar(x, avg_accuracies)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Feature Pairs')\n",
    "    plt.ylabel('Best Accuracy')\n",
    "    plt.title('Best Accuracy for Different Feature Pairs')\n",
    "    plt.xticks(x, [f'{p[0]}\\n{p[1]}' for p in feature_pairs], rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, acc in zip(bars, avg_accuracies):\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width()/2,\n",
    "            bar.get_height(),\n",
    "            f'{acc:.4f}',\n",
    "            ha='center',\n",
    "            va='bottom'\n",
    "        )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('feature_pairs_comparison.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **(Non-mandatory)** Implement a new Class called **FastKNNClassificationModel**. This method should be faster than your regular implementation. This can be done by using a faster data structure to look for the closest neighbors faster. In this assignment, you must build the KDTree with the the training data and then search for the neighbors using it.\n",
    "\n",
    "* You must use this implementation of KDTree from Scipy. https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.KDTree.html\n",
    "* The methods needed for your implementation are only the *constructor* (to build the KDTree) and the method *query* to find the k-neighbors.\n",
    "* You must design an experiment using the **IrisDataset.csv** with **all features** to show that your new implementation is faster than your implementation of **KNNClassificationModel**.\n",
    "* For example, you can measure the time using of each prediction, for each classifier, and plot the average time to give a decision for entries. Also, measure how this would increase/decrease with the increment of the input parameter *k*. \n",
    "* Use a plot(s) from matplotlib to support your answer.\n",
    "\n",
    "**Discuss your findings for this question below**\n",
    "\n",
    "To solve this exercise, I checked and followed this online information: [Geeks for Geeks](https://www.geeksforgeeks.org/how-to-reduce-knn-computation-time-using-kd-tree-or-ball-tree/)\n",
    "\n",
    "\n",
    "KNN has a high computacional cost, specially with large datasets. There are a few methods to reduce this cost, like using efficient data structures; KD-Trees would significantly improve the times, dividing the dataset. \n",
    "\"A KD tree recursively divides the space along alternating dimensions, allowing faster queries with a time complexity of O(logN), compared to the O(N) of brute force.\" \n",
    "\n",
    "I created a class - FastKNNClassificationModel.py -  which creates a KDTree instance, passes the training data, and calls query() method. Then, on the class RunFastKNNClassificationModel I created a method to compare both models for k's values. The models will be compared basede on Time and Accuracy, although the plot only shows the time differences. \n",
    "\n",
    "Based on the output printed on the terminal we can notice that the regular implementation time grows especially after k = 7, while the Fast implementation maintains a with small variations of time. \n",
    "\n",
    "While seeing the generated plot we can see that there's an increasing trend for the Regular KNN, with a spike after k = 5. In contrast, Fast KNN maintains a consistent time accross k values, with minimal increase, concluding that it is more stable compared to Regular KNN. \n",
    "\n",
    "This comparison shows that using KDTree data structure we can increase efficiency of Regular KNN model. \n",
    "\n",
    "![KNN Models Comparison](KNN_Models_Comparison.png)\n",
    "\n",
    "**Please see part of the implementation bellow**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastKNNClassificationModel(MachineLearningModel):\n",
    "    \"\"\"\n",
    "    Fast KNN Classification Model.\n",
    "    KDTree data structure for efficient nearest neighbor search.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        \"\"\"\n",
    "        Initialize the model with k neighbors.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.kdtree = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Build the KDTree with training data.\n",
    "        \"\"\"\n",
    "        # Convert inputs to numpy arrays and ensure correct shape\n",
    "        self.X_train = np.array(X, dtype=float)\n",
    "        if len(self.X_train.shape) == 1:\n",
    "            self.X_train = self.X_train.reshape(-1, 1)\n",
    "            \n",
    "        self.y_train = np.array(y)\n",
    "        \n",
    "        # Here we pass the training data to the KDTree data structure\n",
    "        self.kdtree = KDTree(self.X_train)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict method for test data using KDTree's efficient nearest neighbor search.\n",
    "        \"\"\"\n",
    "        X = np.array(X, dtype=float)\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "            \n",
    "        # Use KDTree to find k nearest neighbors\n",
    "        distances, indices = self.kdtree.query(X, k=self.k)\n",
    "        \n",
    "        predictions = []\n",
    "        for neighbor in indices:\n",
    "            neighbor_labels = self.y_train[neighbor]\n",
    "            most_common = Counter(neighbor_labels).most_common(1)[0][0]\n",
    "            predictions.append(most_common)       \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def evaluate(self, y_true, y_predicted):\n",
    "        \"\"\"\n",
    "        Evaluate method by counting correct predictions.\n",
    "        \"\"\"\n",
    "        y_true = np.array(y_true)\n",
    "        y_predicted = np.array(y_predicted)\n",
    "        return np.sum(y_true == y_predicted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: MNIST k-NN classification (Non-mandatory)\n",
    "\n",
    "In this final exercise, we will use k-NN for classifying handwritten digits using the very famous MNIST dataset. Input to the algorithm is an image (28x28 pixel) with a handwritten digit (0-9) and the output should be a classification 0-9. The dataset and a description of it is available at http://yann.lecun.com/exdb/mnist/. Google MNIST Python to learn how to access it. The objective is to use your k-NN classifier to perform as good as possible on recognizing handwritten images. Describe your effort and what you found out to be the best k to lower the test error. The complete dataset has 60,000 digits for training and 10,000 digits for testing. Hence the computations might be heavy, so start of by a smaller subset rather than using the entire dataset. The final testing should (if possible) be done for the full test set but we will accept solutions that use \"only\" 10,000 digits for training and 1,000 digits for testing.\n",
    "The description of this exercise is deliberately vague as you are supposed to, on your own, find a suitable way to solve this problem in detail. This is why it is important that you document your effort and progress in your report. **You must use your implementations of KNN for classification. If you successfully finished Exercise 3, part 2, it is advisable to use your FastKNNClassificationModel**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem I started by getting the dataset using keras.datasets. I also used as reference this link: [MNIST in Keras](https://colab.research.google.com/github/AviatorMoser/keras-mnist-tutorial/blob/master/MNIST%20in%20Keras.ipynb) \n",
    "\n",
    "Firstly, I tried to visualize the dataset with the prints on lines (8-11). The for loop was just a plot test, that's wjy is commented. \n",
    "\n",
    "Then, with the data normalized I created the KNNClassifier_MNIST class which calls FastKNNClassficationModel - fit, predict, evaluate. \n",
    "\n",
    "When I tried with a train subset of 5000 digits and 1000 for test, these were the results:\n",
    "\n",
    "    Training on subset with  5000  samples\n",
    "    Testing on  1000  samples\n",
    "    Evaluation:  0.912\n",
    "\n",
    "For such a small subset, I think accuracy is quite high. (only for k = 3)\n",
    "\n",
    "Then, I run the same code with different k's:\n",
    "\n",
    "**Testing with k=5**\n",
    "Training on subset with  5000  samples\n",
    "Testing on  1000  samples\n",
    "Accuracy with k=5: 0.9110\n",
    "\n",
    "**Testing with k=7**\n",
    "Training on subset with  5000  samples\n",
    "Testing on  1000  samples\n",
    "Accuracy with k=7: 0.9090\n",
    "\n",
    "**Testing with k=9**\n",
    "Training on subset with  5000  samples\n",
    "Testing on  1000  samples\n",
    "Accuracy with k=9: 0.9120\n",
    "\n",
    "**Testing with k=11**\n",
    "Training on subset with  5000  samples\n",
    "Testing on  1000  samples\n",
    "Accuracy with k=11: 0.8990\n",
    "\n",
    "Based on previous outputs, I believe the best k = 9 would be the best choice.\n",
    "\n",
    "Then, I tried to run with 10,000 digits for training and 1000 for testing, for different k's (took some time to run):\n",
    "\n",
    "**Testing with k=3**\n",
    "Training on subset with  10000  samples\n",
    "Testing on  1000  samples\n",
    "Accuracy with k=3: 0.9220\n",
    "\n",
    "**Testing with k=5**\n",
    "Training on subset with  10000  samples\n",
    "Testing on  1000  samples\n",
    "Accuracy with k=5: 0.9200\n",
    "\n",
    "**Testing with k=7**\n",
    "Training on subset with  10000  samples\n",
    "Testing on  1000  samples\n",
    "Accuracy with k=7: 0.9200\n",
    "\n",
    "**Testing with k=9**\n",
    "Training on subset with  10000  samples\n",
    "Testing on  1000  samples\n",
    "Accuracy with k=9: 0.9160\n",
    "\n",
    "**Testing with k=11**\n",
    "Training on subset with  10000  samples\n",
    "Testing on  1000  samples\n",
    "Accuracy with k=11: 0.9160\n",
    "\n",
    "For this last subset, my conclusion is that the model performs better for a smaller k = 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier_MNIST:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "        self.model = FastKNNClassificationModel(k=self.k)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return self.model.predict(X_test)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        return self.model.evaluate(X_test, y_test)\n",
    "    \n",
    "    def main(self):\n",
    "        # Smaller subset\n",
    "        train = 10000\n",
    "        test = 1000\n",
    "        \n",
    "        k = [3, 5, 7, 9, 11]\n",
    "        # find best k value\n",
    "        for k in k:\n",
    "            knn = KNNClassifier_MNIST(k=k)\n",
    "            print(f\"\\nTesting with k={k}\")\n",
    "            print(\"Training on subset with \", train, \" samples\")\n",
    "            knn.fit(X_train_normalized[:train], train_y[:train])\n",
    "            \n",
    "            print(\"Testing on \", test, \" samples\")\n",
    "            predictions = knn.predict(X_test_normalized[:test])\n",
    "            \n",
    "            accuracy = knn.evaluate(test_y[:test], predictions) / test\n",
    "            print(f\"Accuracy with k={k}: {accuracy:.4f}\")\n",
    "            print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
